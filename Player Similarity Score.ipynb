{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Player Similarity Score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessarry libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Library that allows data manipulation and analysis\n",
    "import numpy as np  # Library for high-level mathematical functions and support for multi-dimensional arrays\n",
    "import matplotlib.pyplot as plt  # Library for plotting\n",
    "import seaborn as sns  # Additional Library for plotting\n",
    "\n",
    "# To import the necessarry libraries and dependendencies required for the machine learning models and their respective training and evaluation.\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    ")\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# To import metrics to measure performance\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option(\"display.max_rows\", None)  # To better display the rows when there are too many\n",
    "pd.set_option(\"display.max_columns\", None)  # To better display the columns when there are too many\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/aleja/OneDrive/Desktop/Data Analytics Msc/Thesis/male_players.csv\") #Loading the CSV file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking general information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True,show_counts=True) # For a summarry of the columns available, the amount of nulls per column and the data type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This initial glimpse of the code allows us to make quick inferences about the dataset being handled. We are dealing with 111 initial columns in our dataset which already shows us that we are probably going to need to perform data cleaning to get rid of unnecesarry columns.\n",
    "\n",
    "- The Non-Null Count also signals that we must perform data cleaning as the count value are not the same for all columns.\n",
    "\n",
    "- Lastly, the Data type of each column tells us how we are going to have to handle each column at the moment of making operations and when considering the steps to handle the different types of variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We run the following method to get a initial look at the data.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This overview of the dataset allows us to understand our next steps for data preprocessing: \n",
    "1. Player positions contain multiple positions separated by a comma. We will have to reduce it to only one so that comparisons can be made properly by avoiding having a mix of multiple positions being recognized as their own category. (This step was completed in Excel using the LEFT and FIND functions, for the sake of including various data analysis tools; hence the player_positions_2 colum).\n",
    "1. Some of the scores assigned to each player for each position in the field (ls,st,rs,lw, etc.) include an operation, i.e. 90+3, which was included to include potential based on certain conditions. This causes the data type to be an object when it would be needed for these colums to be an integer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that will allow us to perform the operations found in some of player position rating columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate the effective overall rating for the player position rankings\n",
    "# Define a function to handle both addition and subtraction\n",
    "def calculate_operation(x):\n",
    "    x = str(x)  # Convert to string to avoid the error\n",
    "    if '+' in x:\n",
    "        return sum(map(int, x.split('+')))\n",
    "    elif '-' in x:\n",
    "        parts = x.split('-')\n",
    "        return int(parts[0]) - int(parts[1])\n",
    "    else:\n",
    "        return int(x)\n",
    "\n",
    "# Apply the function to multiple columns\n",
    "columns_to_process = ['ls', 'st', 'rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk']          \n",
    "\n",
    "# Apply the function to the specified columns and update the DataFrame\n",
    "for col in columns_to_process:\n",
    "    if col in df.columns:  # Check if the column exists in the DataFrame\n",
    "        df[col] = df[col].apply(calculate_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To drop the rows of players missing their market value \n",
    "df = df.dropna(subset=['value_eur'])\n",
    "\n",
    "#To drop the rows of players missing information\n",
    "df = df.dropna(subset=['club_position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We invert the values of ordinal variables like league level so that the logic for all cardinal variable stays consistent; higher is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['league_level'] = 6 - df['league_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By checking the dataset we see that there is a 'player_id' column that holds a unique identifier for each player. We use the following lines of code to check on duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(subset=['player_id' ], keep=False)] #To check if there are any duplicates for player id\n",
    "duplicates = duplicates.sort_values(by=['player_id']) #To sort and show the duplicated together in case there are any\n",
    "duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial to handle missing values as they will skew our results and ML algorithms dont function properly with them. We first analyze the missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df.isnull().sum() #To count the sum of the null values in our dataset\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are columns that have a high percentage of missing values, upwards of 90% in some cases. We decided the best course of action is to get rid of those columns all together and also drop some columns that dont provide valuable information at a first glance. We include some of the financial variables as we know that these dont have an influence over player similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We drop columns that have high null value count and that dont provide much information\n",
    "df = df.drop(columns=['nation_jersey_number','player_traits' , 'goalkeeping_speed',\n",
    "                       'club_loaned_from', 'nation_team_id', 'nation_position',\n",
    "                       'player_tags', 'fifa_update', 'nationality_id', 'real_face',\n",
    "                       'league_id','club_team_id','club_joined_date','update_as_of',\n",
    "                       'dob', 'player_positions', 'body_type','value_eur',\n",
    "                       'wage_eur','release_clause_eur'\n",
    "                       ]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have solved the null values that could be disposed off, we must analyze what procedure to take to solve the null values in the columns that will prove useful to us. We will use histograms to check on the distribution for each of the variables and analyze the best method to impute said columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = df.hist(figsize = (25,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the histograms gave us interesting insights of our data. \n",
    "- The distributions for 'value_eur' and 'wage_eur' present a high degree of skewness to the left, which might affect our data. We will need to apply some transformation to correct this.\n",
    "- The columns 'pace','shooting','passing','dribbling','mentality_composure' all present a normal distribution, which means that null values can be imputed using the mean.\n",
    "- The columns 'defending','physic','release_clause_eur' present skewness in their distributions, meaning that null values can be imputed using the median.\n",
    "- Colums with categorical variables that present null values like 'league_level' can be imputed using the mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To impute the columns that have a normal distribution with the mean\n",
    "df['pace'].fillna(df['pace'].mean(), inplace=True)\n",
    "df['shooting'].fillna(df['shooting'].mean(), inplace=True)\n",
    "df['passing'].fillna(df['passing'].mean(), inplace=True)\n",
    "df['dribbling'].fillna(df['dribbling'].mean(), inplace=True)\n",
    "df['mentality_composure'].fillna(df['mentality_composure'].mean(), inplace=True)\n",
    "\n",
    "#To impute the colums that present skewness with the median\n",
    "df['defending'].fillna(df['defending'].median(), inplace=True)\n",
    "df['physic'].fillna(df['physic'].median(), inplace=True)\n",
    "\n",
    "\n",
    "#To impute the values of a ordinal variable with the mode\n",
    "df['league_level'] = df['league_level'].fillna(df['league_level'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the sum of the null counts to verify that are not any left. ('player_face_url' still presents null values but that is a column that will not be used in the analysis and is only needed for the construction of the Tableau Dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df.isnull().sum() #To count the sum of the null values in our dataset\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking a look a the histograms for each variable we could realize that we would need to apply data transformations techniques that would help us reduce the imbalance in our dataset. The histograms already give us a hint at potential outliers, however we use box plots to confirm take a much more precise look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df.plot(kind='box', figsize=(25, 25), subplots=True, layout=(10, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple conclusions that we can draw from examining the box plots\n",
    "- Some of the variables present a lot of outliers but in some of them it actually makes sense, i.e. height can present outliers on both ends of the box as goalkeepers are usually tall and some regions of the world have shorter players; not necessarily being determinant of player value. This teaches us that we must focus our outlier removal efforts in variables where outliers dont make sense or could potentially affect our results. \n",
    "- Other variables like the goalkeeping score present many outliers as it is logical that field players have low scores in this regard.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling and Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is advisable to put most of the numeric variables on similar scales so that they are more easily comparable. To achieve this we utilize a Robust Scaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We apply Robust Scaler to all of the numerical variables in our dataset\n",
    "\n",
    "columns_to_scale = ['overall','potential','age',\n",
    "                    'height_cm','weight_kg','pace',\n",
    "                    'shooting','passing','dribbling','defending','physic',\n",
    "                    'attacking_crossing','attacking_finishing','attacking_heading_accuracy',\n",
    "                    'attacking_short_passing','attacking_volleys','skill_dribbling','skill_curve',\n",
    "                    'skill_fk_accuracy','skill_long_passing','skill_ball_control','movement_acceleration',\n",
    "                    'movement_sprint_speed','movement_agility','movement_reactions','movement_balance',\n",
    "                    'power_shot_power','power_jumping','power_stamina','power_strength','power_long_shots',\n",
    "                    'mentality_aggression','mentality_interceptions','mentality_positioning',\n",
    "                    'mentality_vision','mentality_penalties','mentality_composure','defending_marking_awareness',\n",
    "                    'defending_standing_tackle','defending_sliding_tackle','goalkeeping_diving',\n",
    "                    'goalkeeping_handling','goalkeeping_kicking','goalkeeping_positioning',\n",
    "                    'goalkeeping_reflexes','ls', 'st', 'rs','lw','lf','cf','rf','rw','lam',\n",
    "                    'cam','ram','lm','lcm','cm','rcm','rm','lwb','ldm','cdm','rdm','rwb','lb',\n",
    "                    'lcb','cb','rcb','rb','gk']\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(df[columns_to_scale])\n",
    "df[columns_to_scale] = pd.DataFrame(scaled_data, index=df.index, columns=columns_to_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables have to be encoded for a better performance with our ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We apply a Hot Encoder to the categorical variables that are not ordinal\n",
    "columns_to_encode = ['player_positions_2','preferred_foot','work_rate','body_type_2']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop=None) \n",
    "encoded_data = encoder.fit_transform(df[columns_to_encode])\n",
    "encoded_columns = encoder.get_feature_names_out(columns_to_encode)\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns, index=df.index)\n",
    "df = pd.concat([df.drop(columns=columns_to_encode), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a list of numerical features so that ML algorithms have no problems processing categorical variables.\n",
    "features = ['overall', 'potential','age', 'height_cm', 'weight_kg', 'club_jersey_number',\n",
    "    'pace', 'shooting', 'passing', 'dribbling', 'defending', \n",
    "    'physic', 'attacking_crossing', 'attacking_finishing', 'attacking_heading_accuracy', \n",
    "    'attacking_short_passing', 'attacking_volleys', 'skill_dribbling', 'skill_curve', \n",
    "    'skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control', 'movement_acceleration', \n",
    "    'movement_sprint_speed', 'movement_agility', 'movement_reactions', 'movement_balance', \n",
    "    'power_shot_power', 'power_jumping', 'power_stamina', 'power_strength', 'power_long_shots', \n",
    "    'mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', \n",
    "    'mentality_penalties', 'mentality_composure', 'defending_marking_awareness', \n",
    "    'defending_standing_tackle', 'defending_sliding_tackle', 'goalkeeping_diving', \n",
    "    'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning', \n",
    "    'goalkeeping_reflexes', 'ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', \n",
    "    'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', \n",
    "    'cb', 'rcb', 'rb', 'gk','league_level','international_reputation'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try different approaches, the first one will be using RFE with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform RFE with Logistic Regression\n",
    "X = df[features]\n",
    "y = df['club_position']\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "rfe = RFE(estimator=model, n_features_to_select=5)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Output selected features\n",
    "selected_features = [features[i] for i in range(len(features)) if rfe.support_[i]]\n",
    "print(\"Top features selected:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second approach will be to use an Extra Trees Classifier to graph the best performing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features]\n",
    "y = df['club_position']\n",
    "\n",
    "# Train an Extra Trees Classifier\n",
    "et_model = ExtraTreesClassifier(random_state=42)\n",
    "et_model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = et_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the sorted feature importances\n",
    "print(importance_df)\n",
    "\n",
    "# Optionally, plot the feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display the highest importance on top\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances - Extra Trees Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our final variables for our model based on the results of the the models but also in conjunction with the recommendations from professional football coaches that formed part of a complementary research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = [\n",
    "    'overall', 'potential','club_jersey_number','pace','shooting','passing','dribbling',\n",
    "    'defending','physic','attacking_crossing', 'attacking_finishing', 'attacking_heading_accuracy', \n",
    "    'attacking_short_passing', 'attacking_volleys', 'skill_dribbling', 'skill_curve', \n",
    "    'skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control', 'movement_acceleration', \n",
    "    'movement_sprint_speed', 'movement_agility', 'movement_reactions', 'movement_balance', \n",
    "    'power_shot_power', 'power_jumping', 'power_stamina', 'power_strength', 'power_long_shots', \n",
    "    'mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', \n",
    "    'mentality_penalties', 'mentality_composure', 'defending_marking_awareness', \n",
    "    'defending_standing_tackle', 'defending_sliding_tackle', 'goalkeeping_diving', \n",
    "    'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning', \n",
    "    'goalkeeping_reflexes', 'ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', \n",
    "    'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', \n",
    "    'cb', 'rcb', 'rb', 'gk','preferred_foot_Right', 'preferred_foot_Left',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining that our dataset will be limited to the year 2024 as we want to compare players to each other and not compare them to other versions of themselves from the previous years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['fifa_version'] == 24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numeric features and player IDs\n",
    "player_features = df[final_features]\n",
    "player_ids = df['player_id']\n",
    "train_data = df[df['fifa_version'] == 23][final_features]  # Data from 2023\n",
    "test_data = df[df['fifa_version'] == 24][final_features]   # Data from 2024\n",
    "train_player_ids = df[df['fifa_version'] == 23]['player_id']\n",
    "test_player_ids = df[df['fifa_version'] == 24]['player_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Create a sparse matrix from df values\n",
    "matrix = csr_matrix(df[final_features].values)\n",
    "\n",
    "# Initialize Nearest Neighbors with cosine similarity and brute force algorithm\n",
    "knn_search = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "knn_search.fit(matrix)\n",
    "\n",
    "# Create a dictionary to store recommendations for each player\n",
    "rec_dict = {}\n",
    "\n",
    "for player_idx, player_row in enumerate(matrix):\n",
    "    # Get 4 neighbors to include the player itself\n",
    "    distances, indices = knn_search.kneighbors(player_row.reshape(1, -1), n_neighbors=4)\n",
    "    \n",
    "    player_id = df.iloc[player_idx]['player_id']  # Get the player_id for the current player\n",
    "    recommendations = []  # Store recommendations for the current player\n",
    "\n",
    "    for elem in range(1, len(distances.flatten())):  # Skip the player itself\n",
    "        rec_player_idx = indices.flatten()[elem]\n",
    "        rec_player_id = df.iloc[rec_player_idx]['player_id']  # Get the recommended player's player_id\n",
    "        rec_player_name = df.iloc[rec_player_idx]['long_name']  # Get the recommended player's name\n",
    "        similarity_score = (1 - distances.flatten()[elem]) #* penalty_factor  # Apply penalty\n",
    "        recommendations.append((rec_player_id, rec_player_name, similarity_score))\n",
    "    \n",
    "    rec_dict[player_id] = recommendations\n",
    "\n",
    "# Create lists for new columns\n",
    "rec_players_1 = []  # First recommended player_id\n",
    "rec_players_2 = []  # Second recommended player_id\n",
    "rec_players_3 = []  # Third recommended player_id\n",
    "similarity_scores_1 = []  # Similarity score for first recommended player\n",
    "similarity_scores_2 = []  # Similarity score for second recommended player\n",
    "similarity_scores_3 = []  # Similarity score for third recommended player\n",
    "rec_player_names_1 = []  # First recommended player name\n",
    "rec_player_names_2 = []  # Second recommended player name\n",
    "rec_player_names_3 = []  # Third recommended player name\n",
    "\n",
    "# Populate new columns with recommended players, their names, and similarity scores\n",
    "for player_id in df['player_id']:\n",
    "    recommendations = rec_dict.get(player_id, [])\n",
    "    recommendations.sort(key=lambda x: x[2], reverse=True)  # Sort by similarity score\n",
    "\n",
    "    if recommendations:\n",
    "        rec_players_1.append(recommendations[0][0])  # Get the first recommended player_id\n",
    "        rec_player_names_1.append(recommendations[0][1])  # Get the first recommended player's name\n",
    "        similarity_scores_1.append(recommendations[0][2])  # Similarity score for the first player\n",
    "\n",
    "        if len(recommendations) > 1:\n",
    "            rec_players_2.append(recommendations[1][0])  # Second recommended player_id\n",
    "            rec_player_names_2.append(recommendations[1][1])  # Second recommended player's name\n",
    "            similarity_scores_2.append(recommendations[1][2])  # Similarity score for the second player\n",
    "\n",
    "        if len(recommendations) > 2:\n",
    "            rec_players_3.append(recommendations[2][0])  # Third recommended player_id\n",
    "            rec_player_names_3.append(recommendations[2][1])  # Third recommended player's name\n",
    "            similarity_scores_3.append(recommendations[2][2])  # Similarity score for the third player\n",
    "        else:\n",
    "            rec_players_3.append(None)\n",
    "            rec_player_names_3.append(None)\n",
    "            similarity_scores_3.append(None)\n",
    "    else:\n",
    "        rec_players_1.append(None)\n",
    "        rec_player_names_1.append(None)\n",
    "        similarity_scores_1.append(None)\n",
    "        rec_players_2.append(None)\n",
    "        rec_player_names_2.append(None)\n",
    "        similarity_scores_2.append(None)\n",
    "        rec_players_3.append(None)\n",
    "        rec_player_names_3.append(None)\n",
    "        similarity_scores_3.append(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Apply SVD to reduce dimensions while maintaining structure\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)  # Reduce to 20 latent features\n",
    "reduced_matrix = svd.fit_transform(df[final_features].values)\n",
    "\n",
    "# Create a dictionary to store recommendations for each player\n",
    "rec_dict = {}\n",
    "\n",
    "for player_idx, player_vec in enumerate(reduced_matrix):\n",
    "    # Get current player's ID and name\n",
    "    current_player_id = df.iloc[player_idx]['player_id']\n",
    "    similarities = []\n",
    "    \n",
    "    for other_idx, other_vec in enumerate(reduced_matrix):\n",
    "        if player_idx != other_idx:\n",
    "            # Calculate similarity score with penalty\n",
    "            similarity_score = (1 - cosine(player_vec, other_vec))\n",
    "            \n",
    "            # Get recommended player's details\n",
    "            rec_player_id = df.iloc[other_idx]['player_id']\n",
    "            rec_player_name = df.iloc[other_idx]['long_name']\n",
    "            \n",
    "            similarities.append((rec_player_id, rec_player_name, similarity_score))\n",
    "    \n",
    "    # Sort by similarity score descending and keep top 3\n",
    "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "    rec_dict[current_player_id] = similarities[:3]\n",
    "\n",
    "# Create lists for new columns (same structure as KNN output)\n",
    "rec_players_1 = []\n",
    "rec_players_2 = []\n",
    "rec_players_3 = []\n",
    "similarity_scores_1 = []\n",
    "similarity_scores_2 = []\n",
    "similarity_scores_3 = []\n",
    "rec_player_names_1 = []\n",
    "rec_player_names_2 = []\n",
    "rec_player_names_3 = []\n",
    "\n",
    "# Populate new columns with recommendations\n",
    "for player_id in df['player_id']:\n",
    "    recommendations = rec_dict.get(player_id, [])\n",
    "    \n",
    "    # Initialize all values as None first\n",
    "    r1, r2, r3 = (None, None, None), (None, None, None), (None, None, None)\n",
    "    \n",
    "    if len(recommendations) >= 1:\n",
    "        r1 = (recommendations[0][0], recommendations[0][1], recommendations[0][2])\n",
    "    if len(recommendations) >= 2:\n",
    "        r2 = (recommendations[1][0], recommendations[1][1], recommendations[1][2])\n",
    "    if len(recommendations) >= 3:\n",
    "        r3 = (recommendations[2][0], recommendations[2][1], recommendations[2][2])\n",
    "    \n",
    "    # Append to lists\n",
    "    rec_players_1.append(r1[0])\n",
    "    rec_player_names_1.append(r1[1])\n",
    "    similarity_scores_1.append(r1[2])\n",
    "    \n",
    "    rec_players_2.append(r2[0])\n",
    "    rec_player_names_2.append(r2[1])\n",
    "    similarity_scores_2.append(r2[2])\n",
    "    \n",
    "    rec_players_3.append(r3[0])\n",
    "    rec_player_names_3.append(r3[1])\n",
    "    similarity_scores_3.append(r3[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the performance of the models in this instance is more complicated than what we did in the market value predictor. There are various methods to measure the similarity of the predicted recommendations. Since we decided to measure the cosine similarity within the models we now have to compute the average of all those cosine similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_similarity(df):\n",
    "    similarity_columns = ['SimilarityScore_1', 'SimilarityScore_2', 'SimilarityScore_3']\n",
    "    \n",
    "    # Convert similarity scores to numeric values (handling None values as 0)\n",
    "    df['AvgSimilarity'] = df[similarity_columns].apply(lambda row: np.nanmean([val for val in row if val is not None]), axis=1)\n",
    "    \n",
    "    return df[['player_id', 'AvgSimilarity']]\n",
    "\n",
    "# Apply function to compute average similarity\n",
    "df_similarity = calculate_average_similarity(df)\n",
    "\n",
    "# Compute the overall average similarity across all players\n",
    "overall_avg_similarity = df_similarity['AvgSimilarity'].mean()\n",
    "\n",
    "print(f\"Overall Average Similarity: {overall_avg_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the results for which model adapts better to our data we can proceed to adding the recommended players with their similarity scores and their IDs to the dataframe so that this can be used in Tableau. To make the process simpler, we reload the original CSV file and append the new columns to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"C:/Users/aleja/OneDrive/Desktop/Data Analytics Msc/Thesis/male_players.csv\") #Loading the CSV file \n",
    "df2 = df2[df2['fifa_version'] == 24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same data preprocessing steps we did before as the dataframe needs to be in the same shape and size for there to be no problems with the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate the effective overall rating for the player position rankings\n",
    "# Define a function to handle both addition and subtraction\n",
    "def calculate_operation(x):\n",
    "    x = str(x)  # Convert to string to avoid the error\n",
    "    if '+' in x:\n",
    "        return sum(map(int, x.split('+')))\n",
    "    elif '-' in x:\n",
    "        parts = x.split('-')\n",
    "        return int(parts[0]) - int(parts[1])\n",
    "    else:\n",
    "        return int(x)\n",
    "\n",
    "# Apply the function to multiple columns\n",
    "columns_to_process = ['ls', 'st', 'rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk']          \n",
    "\n",
    "# Apply the function to the specified columns and update the DataFrame\n",
    "for col in columns_to_process:\n",
    "    if col in df2.columns:  # Check if the column exists in the DataFrame\n",
    "        df2[col] = df2[col].apply(calculate_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To drop the rows of players missing their market value \n",
    "df2 = df2.dropna(subset=['value_eur'])\n",
    "\n",
    "#To drop the rows of players missing information\n",
    "df2 = df2.dropna(subset=['club_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To impute the columns that have a normal distribution with the mean\n",
    "df2['pace'].fillna(df2['pace'].mean(), inplace=True)\n",
    "df2['shooting'].fillna(df2['shooting'].mean(), inplace=True)\n",
    "df2['passing'].fillna(df2['passing'].mean(), inplace=True)\n",
    "df2['dribbling'].fillna(df2['dribbling'].mean(), inplace=True)\n",
    "df2['mentality_composure'].fillna(df2['mentality_composure'].mean(), inplace=True)\n",
    "\n",
    "#To impute the colums that present skewness with the median\n",
    "df2['defending'].fillna(df2['defending'].median(), inplace=True)\n",
    "df2['physic'].fillna(df2['physic'].median(), inplace=True)\n",
    "df2['release_clause_eur'].fillna(df2['release_clause_eur'].median(), inplace=True)\n",
    "\n",
    "#To impute the values of a ordinal variable with the mode\n",
    "df2['league_level'] = df2['league_level'].fillna(df2['league_level'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now append the results of the model executed last to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add new columns to df\n",
    "df2['RecPlayer_1'] = rec_players_1\n",
    "df2['RecPlayerName_1'] = rec_player_names_1\n",
    "df2['SimilarityScore_1'] = similarity_scores_1\n",
    "\n",
    "df2['RecPlayer_2'] = rec_players_2\n",
    "df2['RecPlayerName_2'] = rec_player_names_2\n",
    "df2['SimilarityScore_2'] = similarity_scores_2\n",
    "\n",
    "df2['RecPlayer_3'] = rec_players_3\n",
    "df2['RecPlayerName_3'] = rec_player_names_3\n",
    "df2['SimilarityScore_3'] = similarity_scores_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "df2.to_csv('player_recommendations_with_ids.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
